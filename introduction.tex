\section{Introduction}

\subsection{Robotic manipulation}

Reinforcement Learning (RL) deals with learning what actions to do in order to
reach a long-term goal (a \textit{policy} is learned) and have been widely used
for learning robotic manipulation tasks. Recent research suggests methods
capable of learning real-world robotic manipulation tasks without simulation
pre-training, learning only from real-world experience
\cite{yahya2016collective,gu2016deep,finn2016deep,chebotar2016path}. Using
visual feedback for manipulation tasks is a way to handle unknown poses of
manipulators and target objects, and training these kind of tasks end-to-end
have been successful in simulation tasks
\cite{schulman2015trust,lillicrap2015continuous}. Pose estimation for
real-world robotic manipulation have been shown to work by using convolutional
neural networks (CNN)
\cite{levine2016end,chebotar2016path,yahya2016collective}, although for some
cases it was shown that test-time translations severely affected manipulation
task performance \cite{yahya2016collective}. CNNs have also been trained to
deal with relative poses \cite{park20163d}, and this could be a possible
solution in order to deal with unknown and random camera offsets. Real-world
experiments often rely on human demonstrations to learn a successful policy but
this might not always be available. Recently a successful demonstration of
learning a door opening task from scratch without the need for human
demonstration or simulation pre-training have been shown \cite{gu2016deep}.
This was using a version of Normalized Advantage Function algorithms (NAF)
\cite{gu2016continuous} distributed over several robotic platforms.

\subsection{This study}

NAF have been successful on real-world manipulation tasks, without the need of
human demonstration in contrast to recent research on similar tasks using
Guided Policy Search (GPS) \cite{yahya2016collective}. NAF was also shown to
outperform Deep Deterministic Policy Gradient (DDPG)
\cite{gu2016deep,lillicrap2015continuous}. The formulation of NAF however
assumes a uni-modal shape of the advantage function, while other methods such as
DDPG does not have any restrictions on the functions that can be represented.
In this thesis, a series of experiments were conducted showing that NAF can
learn simpler policies on a distributed real-world robotic setup. However, a
pushing task with a clear multi-modal nature here fails to be learned using NAF
both in a real-world setting, and in simulation. DDPG on the other hand learns
a good policy in simulation and the learned policy is successfully transferred
to the real robotic setup. Pose estimation was done using a CNN in accordance
with previous work \cite{levine2016end,chebotar2016path,yahya2016collective}
and further extended to evaluate whether a proposed pose estimation network can
handle random camera poses.

\subsection{Motivation}

Positive results could imply further automation of manipulation tasks where
relative poses of robots, sensors, and targets are unknown. Using a CNN that
estimates relative poses and that is robust to translations could be
interesting also for researchers and industry beyond the robotics use case as
in this thesis.
