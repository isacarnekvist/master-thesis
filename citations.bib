@article{gu2016deep,
  title={Deep Reinforcement Learning for Robotic Manipulation},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00633},
  year={2016}
}
@article{finn2016deep,
  title={Deep Visual Foresight for Planning Robot Motion},
  author={Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00696},
  year={2016}
}
@article{yahya2016collective,
  title={Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search},
  author={Yahya, Ali and Li, Adrian and Kalakrishnan, Mrinal and Chebotar, Yevgen and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00673},
  year={2016}
}
@article{chebotar2016path,
  title={Path integral guided policy search},
  author={Chebotar, Yevgen and Kalakrishnan, Mrinal and Yahya, Ali and Li, Adrian and Schaal, Stefan and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.00529},
  year={2016}
}
@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={39},
  pages={1--40},
  year={2016}
}
@article{theodorou2010generalized,
  title={A generalized path integral control approach to reinforcement learning},
  author={Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Nov},
  pages={3137--3181},
  year={2010}
}
@inproceedings{montgomery2016guided,
  title={Guided Policy Search via Approximate Mirror Descent},
  author={Montgomery, William H and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4008--4016},
  year={2016}
}
@article{gu2016continuous,
  title={Continuous Deep Q-Learning with Model-based Acceleration},
  author={Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:1603.00748},
  year={2016}
}
@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  year={1998},
  publisher={MIT press Cambridge}
}
@inproceedings{jarrett2009best,
  title={What is the best multi-stage architecture for object recognition?},
  author={Jarrett, Kevin and Kavukcuoglu, Koray and LeCun, Yann and others},
  booktitle={Computer Vision, 2009 IEEE 12th International Conference on},
  pages={2146--2153},
  year={2009},
  organization={IEEE}
}
@inproceedings{schulman2015trust,
  title={Trust Region Policy Optimization.},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael I and Moritz, Philipp},
  booktitle={ICML},
  pages={1889--1897},
  year={2015}
}
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@inproceedings{park20163d,
  title={3D human pose estimation using convolutional neural networks with 2D pose information},
  author={Park, Sungheon and Hwang, Jihye and Kwak, Nojun},
  booktitle={Computer Vision--ECCV 2016 Workshops},
  pages={156--169},
  year={2016},
  organization={Springer}
}
@article{puterman1979convergence,
  title={On the convergence of policy iteration in stationary dynamic programming},
  author={Puterman, Martin L and Brumelle, Shelby L},
  journal={Mathematics of Operations Research},
  volume={4},
  number={1},
  pages={60--69},
  year={1979},
  publisher={INFORMS}
}
@article{sniedovich1986new,
  title={A new look at Bellman's principle of optimality},
  author={Sniedovich, M},
  journal={Journal of Optimization Theory and Applications},
  volume={49},
  number={1},
  pages={161--176},
  year={1986},
  publisher={Springer}
}
