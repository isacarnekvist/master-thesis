\section{Background}

\subsection{Objective}

The interest in conducting this thesis research started with a series of
articles published by researchers at Google in their research blog
\cite{gu2016deep,finn2016deep,yahya2016collective,chebotar2016path}. The main
theme in these articles was robotic manipulation learned by gathering
experience in real time in non-simulated contexts. In two of these articles
\cite{gu2016deep,finn2016deep} tasks are learned from scratch without the need
for initializing by demonstration. Although, in the article by Gu et al.
\cite{gu2016deep}, poses of targets and arms are known from attached equipment.
It would be interesting to incorporate estimation of poses from visual feedback
in this case to lessen the need for external equipment. Another central theme
in these articles is the distributed collection of experience over several
robots. This is done in order to decrease the time it takes to collect data and
to increase variance of the data and the robustness of the algorithms. The use
cases for incorporating and extending these findings could be robotic
manipulation tasks with camera as feedback where exact relative positions of
objects, manipulators, and sensors need not be fixed. Also, where resources
exists to use several robots for speeding up the learning process. Possible
readers might be other researchers working with end-to-end machine learning for
robotic manipulation. Other interested parties might also be manufacturers
where repetitive tasks are a part of the production chain and variations in
these make it hard for robots to be easily programmed for those tasks.

\subsection{Reinforcement learning}

This entire section is descriptions of key concepts from a book on
Reinforcement Learning by Sutton and Barto \cite{sutton1998reinforcement}.

\subsubsection{The three tiers of machine learning}

In reinforcement learning (RL) an agent interacts with an environment and tries
to maximize how much \textit{reward} it can observe from the environment. To
maximize the reward in the long run might require short-time losses, making the
problem more complex than just maximizing for one step at a time. To find a
good strategy, commonly referred to as a \textit{policy}, the agent uses its
experience to make better decisions, this is referred to as
\textit{exploitation}. But, it must also find a balance between exploitation
and to also try out new things, i. e. \textit{exploration}. These things are
specific for RL and therefore distinguishes it from supervised and unsupervised
learning making it the third piece of machine learning.

\subsubsection{Main elements of RL}

Let $S_t$ be the state at time $t$, $R_t$ be the reward at time $t$, and $A_t$
the action at time $t$. The interaction between an agent and its environment in
RL is depicted in figure \ref{fig:rl_flowchart}. At time step $t$, the agent reads
the environment state and takes an action. The environment changes, maybe
stochastically, by responding with a new state and a reward at time $t+1$. 

\begin{figure}[h]
    \centering
    \includegraphics[]{res/agent_environment_interaction.pdf}

    \caption{Agent and environment interaction in RL. $S_t$, $R_t$, and $A_t$
             is the state, reward, and action at time $t$.}

    \label{fig:rl_flowchart}
\end{figure}

The quantity to maximize is often not the individual rewards, but rather the
long term accumulated rewards. Let us call this quantity $G_t$, or
\textit{return}, for an agent at time $t$:

\begin{equation}
    G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{equation}

Some problems imply infinite sequences of actions and rewards and in turn
implying that the sum of all rewards be infinite. It is of course a problem to
maximize something infinite and that is the reason for the $\gamma \in \left[0, 1\right]$ factor above
that alleviates this problem if $\gamma < 1$. For lower values of $\gamma$ the agent
tries to maximize short term rewards, and for larger values long-term rewards. 

A policy is a function from the state of the environment to probabilities over
actions, i. e.  the function that chooses what to do in any situation. Since a
reward is only short-term, a \textit{value function} tries to estimate the
total amount of reward that will be given in the long run for being in some
state and following some policy. To enable planning of actions in the
environment, RL algorithms sometimes uses a \textit{model} in order to try out
actions in this before making decisions. This is usually referred to as
\textit{model-based} RL in contrast to \textit{model-free}. TODO: Change "try
out" above if not correct.

\subsubsection{Finite Markov Decision Processes}

In a RL scenario where the environment has a finite number of states, there is
a finite number of actions, and the Markov property holds is called a
\textit{finite Markov Decision Process} (finite MDP). The dynamics of a finite MDP is completely specified by the
probability distribution:

\begin{equation}
    p(s', r|s, a) = P(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)
\end{equation}

Important functions and terminology that is used throughout RL includes the \textit{state-value function} (abbreviated as value function) and
the \textit{action-value function}. The state-value function with respect to some policy informally tells you
how good a state is to be in given that you follow that policy:

\begin{equation}
    v_\pi(s) = \mathbb{E}_\pi\left[G_t|S_t=s\right] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s\right]
\end{equation}

To compare the value of different actions in some state, given that you thereafter follow some policy $\pi$,
is given by the action-value function:

\begin{equation}
    q_\pi(s, a) = \mathbb{E}_\pi\left[G_t|S_t=s,A_t=a\right] = \mathbb{E}_\pi\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s,A_t=a\right]
\end{equation}

According to RL theory there is always an optimal policy, i. e. that gives the
highest possible expected return given any state. This is often denoted with
$*$ and has the corresponding value and action-value functions $v_*(s)$ and
$q_*(s, a)$. Given the optimal value or action-value function, it is (depending
on the problem) easy to infer the optimal policy, therefore a common approach
is to first approximate either of these functions.

\subsubsection{Policy and value iteration}

One exact method to find the optimal policy, at least in the limit, is called
\textit{policy iteration}. This builds on two alternating steps, the first
called \textit{iterative policy evaluation}. This estimates a value function
given some policy and starts from a random value function $v_0$, except for any
terminal state which is assigned $0$. Then we iteratively update new value
functions for each step:

\begin{align*}
    v_{k+1}(s) &= \mathbb{E}\left[R_{t+1} + \gamma v_{k}(S_{t+1}) | S_t=s \right] \\
               &= \sum_a \pi (a|s) \sum p(s', r|s, a) \left[r + \gamma v_k(s')\right]
\end{align*}

As can be seen, the dynamics $p(s', r|s, a)$ needs to be known, which of course
is not always the case. The next step is called \textit{policy improvement} and for this
we first need to calculate the action-state function given the current policy
$\pi$:

\begin{align*}
    q_\pi(s, a) &= \mathbb{E}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t = a \right] \\
                &= \sum_{s', r} p(s', r|s, a) \left[r + \gamma v_\pi(s')\right]
\end{align*}

When we have this, we can get an improved policy $\pi'$ by:

\begin{equation}
    \pi'(s) = \text{arg}\max_a q_\pi(s, a)
\end{equation}

Iteratively doing these two steps will eventually converge to the optimal
policy. There is an alternative way that is done by only approximating the
value function, called \textit{value iteration}:

\begin{align*}
    v_{k+1}(s) &= \max_a \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s, A_t = a \right] \\
             &= \max_a \sum_{s', r} p(s', r|s, a) \left[r + \gamma v_k(s')\right]
\end{align*}

\subsubsection{Monte Carlo methods and Time Difference learning}

Policy and value iteration are exploring the entire state-action space and
finds an optimal policy if the dynamics of the environment are known. Sometimes
we are dealing with samples from interacting with a system, and where we do not
know the dynamics. For these cases, we can instead estimate the action-value
function given a policy. This can be done by \textit{Monte Carlo methods} which
basically is averaging of returns for samples that we have attained. The other
method is \textit{Time difference methods} which estimates an error for each
observed reward and updates the action-value function with this. To be more
precise, one famous example of a time difference method is \textit{Q-learning}
and the updates are done according to:

\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right]
\end{equation}

Q-learning is an example of an \textit{off-policy} method. This means that you
can use a second, or derived, policy for exploration, but the algorithm still
finds the greedy optimal policy. The other family of methods is called
\textit{on-policy} methods. In this case, the exploring policy is the
same as the policy being improved.

\subsection{Extending Q-learning to continuous domains}

The Q-learning method assumes you can keep a finite set of estimates for all
state-action pairs. Continuous state and action spaces require other methods,
or modifications to Q-learning to be possible to use. Gu et al.
\cite{gu2016continuous} proposes a relatively simple variant of Q-learning they
call normalized advantage functions (NAF). They argue after doing simulation
experiments that this algorithm is an effective alternative to recently
proposed actor-critic methods and that it learns faster with more accurate
resulting policies. First, the action-value function is divided into a sum of
the value function $V$ and what they call an advantage function $A$:

\begin{equation}
    Q(\mathbf{x}, \mathbf{u}) = A(\mathbf{x}, \mathbf{u}) + V(\mathbf{x})
\end{equation}

Here, $\mathbf{x}$ is the state of the environment and $\mathbf{u}$ are
controls or actions. The advantage function is a quadratic function of $u$:

\begin{equation}
    A(\mathbf{x}, \mathbf{u}) = -\frac{1}{2}(\mathbf{u} - \mathbf{\mu(x)})^T\mathbf{P(x)}(\mathbf{u} - \mathbf{\mu(x)})
    \label{eq:advantage_function}
\end{equation}

There are more terms that need to be defined, but let's look at equation
\ref{eq:advantage_function} first. The matrix $\mathbf{P}$ is a
positive-definite matrix, this makes the advantage function have its maximum
when $\mathbf{u = \mu(x)}$.  The purpose of $\mu$ is to be a greedy policy
function, thus $\mathbf{Q}$ is maximized when $\mathbf{u}$ is the greedy
action. The purpose of this is that given an optimal $Q$, we do not need to
search for the optimal action, since we know $\mathbf{\mu}$. Now the definition
of $\mathbf{P}$:

\begin{equation}
    P(\mathbf{x}) = \mathbf{L(x)L(x)}^T
\end{equation}

Here, $\mathbf{L}$ is a lower-triangular matrix where diagonal entries are
strictly positive.

After these definitions, we are left with estimating the functions $V$,
$\mathbf{\mu}$, and $\mathbf{L}$. To this end the authors use a neural network,
here shown in figure \ref{fig:naf-net}. The $\mathbf{L}$ output is fully
connected with the previous layer and not passed through an activation function
(it is linear). The diagonal entries of L are exponentiated. Hidden layers
consisted of 200 fully connected units with rectified linear units (ReLU) as
activation functions except for $\mathbf{L}$ and $A$ as already defined.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{res/naf-net.pdf}

    \caption{Neural network design for NAF \cite{gu2016continuous}. The
    activation function to $V$ was not specified. The tanh activation was added
    in figure due to Gu et al. \cite{gu2016deep} using it in order to have
    bounded actions for safety reasons.}

    \label{fig:naf-net}
\end{figure}

The NAF algorithm is listed in algorithm \ref{algo:naf}. All collected experiences
are stored in a replay buffer that optimization is run against. Exploration is done
by adding noise to the current greedy policy $\mathbf{\mu}$.

\begin{algorithm}[!h]
    \caption{NAF algorithm}
    \begin{algorithmic}
        \STATE{Randomly initialize network $Q(x, u|\theta)$}
        \STATE{Initialize target network $Q'$, $\theta' \leftarrow \theta$}
        \STATE{Initialize replay buffer $R \leftarrow \emptyset$}
        \FOR{episode $= 1$ to $M$}
            \STATE{Initialize random process $\mathcal{N}$ for action exploration}
            \STATE{Receive initial exploration state $x_1$}
            \FOR{$t = 1$ to $T$}
                \STATE{Select action $\mathbf{u_t = \mu(x_t|\theta) + \mathcal{N}_t}$}
                \STATE{Execute $\mathbf{u_t}$ and observe $r_t$ and $\mathbf{x}_{t+1}$}
                \STATE{Store transition $(\mathbf{x}_t, \mathbf{u}_t, r_t, \mathbf{x}_{t+1})$ in $R$}
                \FOR{iteration $= 1$ to $I$}
                    \STATE{Sample a random minibatch of $m$ transitions from $R$}
                    \STATE{Set $y_i = r_i + \gamma V'(\mathbf{x}_{i+1}|\theta')$}
                    \STATE{Update $\theta$ by minimizing the loss:
                           $L = \frac{1}{N}\sum_i (y_i - Q(\mathbf{x}_i, \mathbf{u}_i|\theta))^2$}
                    \STATE{Update target network $\theta' \leftarrow \tau \theta + (1 - \tau)\theta'$}
                \ENDFOR
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
    \label{algo:naf}
\end{algorithm}

\subsection{Distributed real-world learning using NAF}

Real-world experiments were done by Gu et al. \cite{gu2016deep} on door opening
tasks using the NAF algorithm and 7-DoF torque controlled arms. They extended
the algorithm to be distributed on several robots/collectors and one separate
trainer thread on a separate machine. They state that this was the first
successful real-world experiment with a relatively high complexity problem
without human demonstration or simulated pretraining. They used the layout of
the network shown in figure \ref{fig:naf-net} but with hidden layers of 100
units each. Also, in this article it was explicitly mentioned that the
activation functions for the policy $\mathbf{\mu}$ was tanh in order to bound
the actions. The state input consisted of the arm pose and target pose. The
target pose was known from attached equipment. The modified version of NAF is
listed in algorithm \ref{algo:async_naf}.

The authors conclude that there was an upper bound on the effects of
parallelization, but hypothesize that the speed of the trainer thread has a
limiting factor in this matter. They used CPU for training the neural network
so instead using a GPU might increase the effect of more collectors.

\begin{algorithm}[!h]
    \caption{Asynchronous NAF - $N$ collector threads and $1$ trainer thread}
    \begin{algorithmic}
        \STATE{// trainer thread}
        \STATE{Randomly initialize network $Q(\mathbf{x}, \mathbf{u}|\theta)$}
        \STATE{Initialize target network $Q'$, $\theta' \leftarrow \theta$}
        \STATE{Initialize shared replay buffer $R \leftarrow \emptyset$}
        \FOR{iteration $= 1$ to $I$}
            \STATE{Sample a random minibatch of m transitions from $R$}
            \STATE{Set $y_i = \begin{cases}r_i+\gamma V'(x_i|\theta) &\text{ if } t_i < T,\\r_i &\text{ if } t_i = T\end{cases}$}
            \STATE{Update $\theta$ by minimizing the loss:
                   $L = \frac{1}{m} \sum_i (y_i - Q(\mathbf{x}_i, \mathbf{u}_i|\theta))^2$}
            \STATE{Update the target network: $\theta' \leftarrow \tau\theta + (1-\tau)\theta'$}
        \ENDFOR
        \STATE{// collector thread $n$, $n = 1...N$}
        \FOR{episode $= 1$ to $M$}
            \STATE{Sync policy network weights $\theta_n \leftarrow \theta$}
            \STATE{Initialize a random process $\mathcal{N}$ for action exploration}
            \STATE{Receive initial observation state $x_1$}
            \FOR{$t = 1$ to $T$}
                \STATE{Select action $\mathbf{u}_t = \mathbf{mu}$}
                \STATE{Execute $\mathbf{u}_t$ and observe $r_t$ and $\mathbf{x}_{t+1}$}
                \STATE{Send transition $(\mathbf{x}_t, \mathbf{u}_t, r_t, \mathbf{x}_{t+1}, t)$ to $R$}
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
    \label{algo:async_naf}
\end{algorithm}

\subsection{Door opening with human demonstration}

Chebotar et al. \cite{chebotar2016path} demonstrates door opening tasks
initialized from human demonstration using methods called Guided Policy Search
(GPS) and Policy Improvement with Path Integrals (PI$^2$). They demonstrate
that they can learn this task outputting torque control from visual input. This is
extended by Yahya et al. \cite{yahya2016collective} to be trained
simultaneously on several robots. In both cases, torque commands are the output
of a neural network where the first part takes visual input and is pretrained
with pose estimates of the door and the robot arm. This neural network is shown
in figure \ref{fig:gps_net}. Since the robot poses were labeled in the robot
base frame and the camera offsets were somewhat random, a linear mapping from
the neural network output to the base frame was learned separately for each of
the robots. The authors evaluated the trained policy on a previously unseen
door with different translations and rotations. The mean success rate was
90\%. They also tried offsetting the camera 1) 5 cm towards the ground and 2) 4
cm backwards. The respective success rates for these translations were 52 \%
and 54 \%.

\begin{figure}[h]
    \centering
    \includegraphics[width = 1.0\textwidth]{res/gps-net.pdf}
    \caption{Network used for door opening task from visual inputs \cite{chebotar2016path}}
    \label{fig:gps_net}
\end{figure}

\subsection{Motion planning by a predictive model}

Another method for learning manipulation tasks was shown by Finn and Levine
\cite{finn2016deep}. Given a sequence of initial images, they denote it
$I_{0:1}$, together with robot poses $x_{0:1}$, and future commands $a_{1:H_p}$
where $H_p$ is some prediction horizon, they predict the distribution of future
images $I_{2:H_p + 1}$. $I_t$ is here a collection of independent pixel
distributions with mean $\hat{I}_t$. A convolutional LSTM was used for this,
and an implicit feature of the network is what they call a flow map
$\hat{F}_t(x, y, k, l)$ which is the probability of the pixel at coordinates
$(k, l)$ at time $t+1$ originating from coordinates $(x, y)$. Given a flow map
they can do one step prediction:

\begin{equation}
    \hat{F}_t \odot \hat{I}_t := \sum_{k \in (-\kappa, \kappa)} \sum_{l \in (-\kappa, \kappa)} \hat{F}_t(x,y,k,l)\hat{I}_t(x-k, y-l) = \hat{I}_{t+1}
\end{equation}

When using these implicit flow operators rather than actual predicted image
which was the output from the network, they can predict movement of individual
pixels for several time steps ahead. Using a maximum likelihood approach, they
can find the series of actions that puts a pixel at some other target location.
For evaluation, they try their trained algorithm on previously unseen objects
by selecting start and end coordinates for which it generalized.

An interesting feature of their approach is that the data set for training was
generated by torque-controlled arms randomly interacting with a collection of
objects and recording images of it along with the robot poses. Therefore, no
part of the training needed human supervision or demonstration. Another feature
is that the camera positions were not the same across robots, it is unclear if
they evaluated the method with a new camera position though.

\subsection{Short on neural networks}

\subsubsection{Basic idea}

The simplest neural network could be considered to be a linear transformation
of some data points $X$:

\begin{equation}
    X_1 = WX + B
\end{equation}

Since for some column in $X$, all its values influence all the values in the
same column in $X_1$, the ''input layer'' $X$ and ''layer'' $X_1$ is said to be
\textit{fully connected}. This is obviously restricted to learning linear
transformations, so to learn non-linear functions, you first add a non-linear
\textit{activation-function}:

\begin{equation}
    X_1 = f(WX + B)
\end{equation}

And then, we can recursively stack transformations in order to learn more complex
functions:

\begin{align}
    X_2 &= f(W_1X_1 + B_1)\\
    ...\\
    Y_{k+1} &= f(W_kX_k + B_k)
\end{align}

By all parts being differentiable we can specify a loss function and calculate
its derivatives with respect to all the parameters $W, W_1,...$ and $B, B_1,
...$. Using this we can then minimize the loss using gradient descent. The first
layer $X$ is commonly referred to as the input layer and the last layer as output
layer. The intermediate values are referred to as hidden layers.

\subsubsection{Common activation functions}

Two common activation functions include \textit{tanh} and \textit{Rectified
Linear Unit} (ReLU) shown in figure \ref{fig:tanh_relu}. The tanh function is defined as:

\begin{equation}
    \tanh(x) = \frac{2}{1+e^{-x}} - 1
\end{equation}

The ReLU is defined as:

\begin{equation}
    \text{relu}(x) = \max (0, x)
\end{equation}

For classification networks, a common function to have in the output layer is
the softmax function shown in figure \ref{fig:softmax}. The softmax have the
property that all values will be output in the range $[0, 1]$ and sum to one,
making it conveniently interpreted as probabilities of different classes or
categories. The mathematical definition is for a set $x_1, ..., x_K$:

\begin{equation}
    \text{softmax}(x_k|x_1, ..., x_K) = \frac{e^{x_k}}{\sum_{i=1}^K e^{x_i}}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{res/relu_tanh.pdf}

    \caption{Common activation functions for neural networks.}

    \label{fig:tanh_relu}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{res/softmax.pdf}

    \caption{Softmax function commonly used to convert a set of numbers $\in \mathbb{R}$ to probabilities of a discrete set of categories.}

    \label{fig:softmax}
\end{figure}

\subsubsection{Weight sharing}
In order to diminish the number of parameters and use the fact that some features are invariant of where in the data they are,
e.g. the pattern $1, 2, 3, 4$ in the beginning, middle, or the end of the data, one often uses convolutions:

\begin{equation}
    y = w * x + b
\end{equation}

The notation becomes a bit more tricky here since the convolutions are commonly done in two dimensions with
3-dimensional kernels ($w$). These are commonly used in neural networks for images.

TODO: Find and add about spatial softmax?

\subsection{Problem statement}

Manipulation tasks that seem trivial to a human can be hard to learn for
robots, especially from scratch without initial human demonstration due to high
sample complexity. Recent research suggests ways to do this but are based on
that you know the poses of the objects and the end-effector. For some scenarios
these are non-trivial to find out.

Problems also arise when learning in real time by collecting experience. Robots
must be able to evaluate their policies regularly at a high rate which is
complicated by adding a deep convolutional neural network for pose detection.
Also, learning tasks within a feasible time frame is harder when data
collection and policy updates happen in real time. The approach of
distributing collection of experience over several robots will be evaluated in
this thesis for handling this problem.

\subsection{Research question}

How can deep and distributed reinforcement learning be used for learning and
performing dynamic manipulation tasks with unknown poses.

\subsection{Expected scientific results}

If all goes well, previous results are verified in new contexts. Also
they are extended to also handle unknown target and manipulator poses.
