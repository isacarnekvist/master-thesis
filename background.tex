\section{Background}

\subsection{Objective}

The interest in conducting this thesis research started with a series of
articles published by researchers at Google in their research blog
\cite{gu2016deep,finn2016deep,yahya2016collective,chebotar2016path}. The main
theme in these articles was robotic manipulation learned by gathering
experience in real time in non-simulated contexts. These articles will be
presented in more detail below and extended during the pilot study. In two of
these articles \cite{gu2016deep,finn2016deep} tasks are learned from scratch
without the need for initializing by demonstration. Although, in the article by
Gu et al. \cite{gu2016deep}, poses of targets and arms are known by attached
equipment. It would be interesting to incorporate estimation of poses from
visual feedback in this case to lessen the need for external equipment. Another
central theme in these articles is the distributed collection of experience
over several robots. This is done in order to decrease the time it takes to
collect data and to increase variance of the data. The use cases for
incorporating and extending these findings could be robotic manipulation tasks
with camera as feedback where exact relative positions of objects,
manipulators, and sensors need not be fixed. Also, where resources exists to
use several robots for speeding up the learning process. Possible readers might
be other researchers working with end-to-end machine learning for robotic
manipulation. Other interested parties might also be manufacturers where
repetitive tasks are a part of the production chain and variations in these
make it hard for robots to be easily programmed for those tasks.

\subsection{Pilot study}

The following sections are the preliminary sources of information that was the
initial spark for this thesis as mentioned above. How to re-implement these
articles is not self-contained, so the pilot would necessarily need to also
include reading into articles from the references of these. Reading of these
initial articles would be needed to motivate an appropriate method, and then
further research would be done with the purpose of gaining all the information
needed to implement such a solution. The thesis study will be conducted at the
Robotics, Perception, and Learning lab at KTH with the main interest originally
being to dig into these articles and develop something further.

\subsubsection{Motion planning by ''Deep visual foresight''}

This article \cite{finn2016deep} trains a convolutional neural network on
images together with motion as inputs to predict how the image will change due
to that motion. This is later used to plan movement of objects to some target
pose.

\subsubsection{Path Integral Guided Policy Search}

In this article \cite{chebotar2016path}, the authors extend Guided Policy
Search and demonstrate two manipulation tasks. These are initialized from
demonstrations. To be able to comprehend this article, referenced articles
\cite{levine2016end,theodorou2010generalized,montgomery2016guided} would have
to be read as well.

\subsubsection{Collective Robot Reinforcement Learning with Distributed
               Asynchronous Guided Policy Search}

This article \cite{yahya2016collective} distributes learning of
door opening across several robots. The exact nature of the tasks
are varied across robots to increase robustness. The learning is
initialized from demonstration.

\subsubsection{Parallelized training without prior demonstration}

This article \cite{gu2016deep} shows several robotic manipulation tasks where
learning is parallelized across platforms, and they do not require previous
demonstrations.  For this article, I would need to read up on an algorithm
called Normalized Advantage Function (NAF) \cite{gu2016continuous}. In both of
the two previously mentioned articles
\cite{chebotar2016path,yahya2016collective} pose estimation of targets and
robots are done through visual feedback, while in this article
\cite{gu2016deep} no sensory feedback is provided and poses are known through
attached equipment. The pose estimation was done using a convolutional neural
network which could be a feasible extension to this article.


\subsubsection{Reinforcement learning}

These articles mentioned above naturally deals with reinforcement vocabulary
and assumes knowledge in this area. Therefore the pilot would include studying
a book by Sutton and Barto \cite{sutton1998reinforcement}. In this book,
chapters 1-3 and a section about non-linear function approximators are
essential (by advice from supervisor).

\subsection{Problem statement}

Manipulation tasks that seem trivial to a human can be hard to learn for
robots, especially from scratch without initial human demonstration due to high
sample complexity. Recent research suggests ways to do this but are based on
that you know the poses of the objects and the end-effector. For some scenarios
these are non-trivial to find out.

Problems also arise when learning in real time by collecting experience. Robots
must be able to evaluate their policies regularly at a high rate which is
complicated by adding a deep convolutional neural network for pose detection.
Also, learning tasks within a feasible time frame is harder when data
collection and policy updates happen in real time. The approach of
distributing collection of experience over several robots will be evaluated in
this thesis for handling this problem.

\subsection{Research question}

How can deep and distributed reinforcement learning be used for learning and
performing dynamic manipulation tasks with unknown poses.

\subsection{Expected scientific results}

If all goes well, previous results are verified in new contexts. Also
they are extended to also handle unknown target and manipulator poses.
